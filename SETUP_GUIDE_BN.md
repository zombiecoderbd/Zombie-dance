# ZombieCoder AI Assistant - рж╕рзЗржЯржЖржк ржЧрж╛ржЗржб

## ЁЯОп рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржмрж┐ржмрж░ржг

**ZombieCoder** ржПржХржЯрж┐ рж╕ржорзНржкрзВрж░рзНржг local-first, OpenAI-compatible AI code assistant ржпрж╛ ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛ржпрж╝ ржХрж╛ржЬ ржХрж░рзЗред ржПржЯрж┐ Ollama ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ рж╕ржорзНржкрзВрж░рзНржг offline ржХрж╛ржЬ ржХрж░рждрзЗ ржкрж╛рж░рзЗ ржПржмржВ ржпрзЗржХрзЛржирзЛ editor (VS Code, Cursor, Continue, etc.) ржП ржХрж╛ржЬ ржХрж░ржмрзЗ ржпрзЗржЧрзБрж▓рзЛ OpenAI API support ржХрж░рзЗред

**ржорзВрж▓ ржмрзИрж╢рж┐рж╖рзНржЯрзНржп**:
- тЬЕ OpenAI API рж╕ржорзНржкрзВрж░рзНржг compatible
- тЬЕ Fake model names (gpt-4, claude-3, etc.) support
- тЬЕ Streaming ржПржмржВ non-streaming
- тЬЕ WebSocket support
- тЬЕ Session management
- тЬЕ RAG (Retrieval-Augmented Generation)
- тЬЕ рж╕ржорзНржкрзВрж░рзНржг ржмрж╛ржВрж▓рж╛ support
- тЬЕ Privacy-focused (рж╕ржм data local)

---

## ЁЯУЛ ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝ Software

### рзз. Node.js (ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝)
- **Version**: 18.x ржмрж╛ рждрж╛рж░ ржмрзЗрж╢рж┐
- **Download**: https://nodejs.org/
- **ржпрж╛ржЪрж╛ржЗ ржХрж░рзБржи**: `node --version`

### рзи. Ollama (ржкрзНрж░ржпрж╝рзЛржЬржирзАржпрж╝)
- **Download**: https://ollama.ai/
- **Install ржХрж░рж╛рж░ ржкрж░**: `ollama serve` command ржжрж┐ржпрж╝рзЗ start ржХрж░рзБржи
- **ржпрж╛ржЪрж╛ржЗ ржХрж░рзБржи**: `ollama list`

### рзй. Git (optional)
- **Download**: https://git-scm.com/

---

## ЁЯЪА Quick Start (ржжрзНрж░рзБржд рж╢рзБрж░рзБ)

### ржзрж╛ржк рзз: Model Download ржХрж░рзБржи

```bash
# Default ржПржмржВ ржжрзНрж░рзБрждрждржо model
ollama pull qwen2.5-coder:0.5b

# ржЕрждрж┐рж░рж┐ржХрзНржд models (optional)
ollama pull qwen2.5-coder:1.5b
ollama pull deepseek-r1:1.5b
ollama pull deepseek-coder:1.3b
```

### ржзрж╛ржк рзи: Database Initialize ржХрж░рзБржи

```bash
node backend/init-db-fixed.cjs
```

**Expected Output**:
```
тЬЕ Database initialization complete!
ЁЯОЙ Imported 12 new Ollama models
```

### ржзрж╛ржк рзй: Backend Build ржХрж░рзБржи

```bash
cd backend
npm run build
cd ..
```

### ржзрж╛ржк рзк: рж╕ржм ржХрж┐ржЫрзБ ржПржХрж╕рж╛ржерзЗ Start ржХрж░рзБржи

#### Windows:
```bash
start-zombiecoder.bat
```

#### Linux/Mac:
```bash
# Backend
cd backend && npm run dev &

# Proxy (ржирждрзБржи terminal ржП)
npm run proxy &
```

### ржзрж╛ржк рзл: ржпрж╛ржЪрж╛ржЗ ржХрж░рзБржи

```bash
# Health check
curl http://localhost:8001/v1/health

# Test chat
curl -X POST http://localhost:5010/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d "{\"model\":\"gpt-3.5-turbo\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello\"}]}"
```

---

## ЁЯФз Configuration

### Main Configuration File: `zombiecoder-config.json`

```json
{
  "server": {
    "ports": {
      "backend": 8001,
      "proxy": 5010
    }
  },
  "models": {
    "default": "qwen2.5-coder:0.5b",
    "aliases": {
      "gpt-4": "qwen2.5-coder:1.5b",
      "gpt-3.5-turbo": "qwen2.5-coder:0.5b",
      "claude-3-opus": "deepseek-coder:1.3b"
    }
  }
}
```

### Environment Variables (.env file)

```env
# Backend Settings
PORT=8001
NODE_ENV=production

# Ollama Settings
OLLAMA_HOST=http://localhost:11434

# Model Settings
DEFAULT_MODEL=qwen2.5-coder:0.5b
TEMPERATURE=0.7
MAX_TOKENS=4096

# Timeouts
STREAM_TIMEOUT=30000
REQUEST_TIMEOUT=30000

# CORS
CORS_ORIGINS=*

# Logging
LOG_LEVEL=info
```

---

## ЁЯдЦ Model Mapping (Fake Names)

ржпрзЗржХрзЛржирзЛ editor ржпржжрж┐ рж╢рзБржзрзБржорж╛рждрзНрж░ OpenAI/Anthropic model names support ржХрж░рзЗ, рждрж╛рж╣рж▓рзЗ ржПржЗ fake names ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи:

### OpenAI Models тЖТ Real Ollama Models

| Fake Name | Real Ollama Model | Speed | Quality |
|-----------|-------------------|-------|---------|
| `gpt-3.5-turbo` | `qwen2.5-coder:0.5b` | тЪбтЪбтЪб ржжрзНрж░рзБрждрждржо | тнРтнРтнР ржнрж╛рж▓рзЛ |
| `gpt-4` | `qwen2.5-coder:1.5b` | тЪбтЪб ржорж╛ржЭрж╛рж░рж┐ | тнРтнРтнРтнР ржЪржорзОржХрж╛рж░ |
| `gpt-4-turbo` | `deepseek-r1:1.5b` | тЪбтЪб ржорж╛ржЭрж╛рж░рж┐ | тнРтнРтнРтнР ржЪржорзОржХрж╛рж░ |
| `gpt-4o` | `qwen2.5-coder:1.5b` | тЪбтЪб ржорж╛ржЭрж╛рж░рж┐ | тнРтнРтнРтнР ржЪржорзОржХрж╛рж░ |
| `gpt-4o-mini` | `qwen2.5-coder:0.5b` | тЪбтЪбтЪб ржжрзНрж░рзБрждрждржо | тнРтнРтнР ржнрж╛рж▓рзЛ |

### Anthropic Models тЖТ Real Ollama Models

| Fake Name | Real Ollama Model | Speed | Quality |
|-----------|-------------------|-------|---------|
| `claude-3-opus` | `deepseek-coder:1.3b` | тЪбтЪб ржорж╛ржЭрж╛рж░рж┐ | тнРтнРтнРтнР ржЪржорзОржХрж╛рж░ |
| `claude-3-sonnet` | `qwen2.5-coder:1.5b` | тЪбтЪб ржорж╛ржЭрж╛рж░рж┐ | тнРтнРтнРтнР ржЪржорзОржХрж╛рж░ |
| `claude-3-haiku` | `qwen2.5-coder:0.5b` | тЪбтЪбтЪб ржжрзНрж░рзБрждрждржо | тнРтнРтнР ржнрж╛рж▓рзЛ |

### Recommended Models

**ржжрзНрж░рзБржд response ржПрж░ ржЬржирзНржп**: `gpt-3.5-turbo` ржмрж╛ `gpt-4o-mini`  
**ржнрж╛рж▓рзЛ quality ржПрж░ ржЬржирзНржп**: `gpt-4` ржмрж╛ `claude-3-sonnet`  
**reasoning ржПрж░ ржЬржирзНржп**: `gpt-4-turbo`

---

## ЁЯФМ Editor Integration

### VS Code / Cursor / Continue

#### Method 1: OpenAI API Compatible

**settings.json**:
```json
{
  "continue.modelProvider": "openai",
  "continue.apiBase": "http://localhost:5010/v1",
  "continue.model": "gpt-3.5-turbo",
  "continue.apiKey": "sk-dummy-key"
}
```

#### Method 2: Custom Provider

**config.json** (Continue extension):
```json
{
  "models": [
    {
      "title": "ZombieCoder GPT-3.5",
      "provider": "openai",
      "model": "gpt-3.5-turbo",
      "apiBase": "http://localhost:5010/v1",
      "apiKey": "sk-dummy"
    },
    {
      "title": "ZombieCoder GPT-4",
      "provider": "openai",
      "model": "gpt-4",
      "apiBase": "http://localhost:5010/v1",
      "apiKey": "sk-dummy"
    }
  ]
}
```

### Cline / Aider / Other Editors

ржпрзЗржХрзЛржирзЛ tool ржпрж╛ OpenAI API support ржХрж░рзЗ:

```bash
# Set environment variables
export OPENAI_API_BASE="http://localhost:5010/v1"
export OPENAI_API_KEY="sk-dummy-key"

# Use with any model name
# The system will automatically map fake names to real models
```

---

## ЁЯУК Available Endpoints

### Backend (Port 8001)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/health` | GET | Health check |
| `/v1/chat` | POST | Direct chat (non-streaming) |
| `/v1/chat/stream` | POST | Streaming chat |
| `/v1/chat/ws` | WS | WebSocket connection |
| `/api/models` | GET | Available models |

### Proxy (Port 5010) - OpenAI Compatible

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/chat/completions` | POST | OpenAI-compatible chat |
| `/v1/models` | GET | OpenAI-compatible models list |
| `/proxy/health` | GET | Proxy health check |

### Example Requests

#### 1. OpenAI-Compatible Chat (Non-Streaming)

```bash
curl -X POST http://localhost:5010/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ]
  }'
```

#### 2. OpenAI-Compatible Chat (Streaming)

```bash
curl -X POST http://localhost:5010/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [
      {"role": "user", "content": "Count from 1 to 5"}
    ],
    "stream": true
  }'
```

#### 3. Direct Backend Chat

```bash
curl -X POST http://localhost:8001/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Write a hello world in Python",
    "context": {},
    "model": "qwen2.5-coder:0.5b"
  }'
```

#### 4. Get Available Models

```bash
curl http://localhost:5010/v1/models
```

---

## ЁЯФН Troubleshooting (рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи)

### рж╕ржорж╕рзНржпрж╛ рзз: Backend start рж╣ржЪрзНржЫрзЗ ржирж╛

**Check ржХрж░рзБржи**:
```bash
# Port already in use?
netstat -ano | findstr :8001

# Kill process
taskkill /F /PID <PID>
```

**рж╕ржорж╛ржзрж╛ржи**:
- Database re-initialize ржХрж░рзБржи: `node backend/init-db-fixed.cjs`
- Backend rebuild ржХрж░рзБржи: `cd backend && npm run build`

---

### рж╕ржорж╕рзНржпрж╛ рзи: "Model not found" error

**Check ржХрж░рзБржи**:
```bash
# Ollama running ржХрж┐ржирж╛
curl http://localhost:11434/api/tags

# Models list ржжрзЗржЦрзБржи
ollama list
```

**рж╕ржорж╛ржзрж╛ржи**:
```bash
# Default model pull ржХрж░рзБржи
ollama pull qwen2.5-coder:0.5b

# Database re-initialize ржХрж░рзБржи
node backend/init-db-fixed.cjs
```

---

### рж╕ржорж╕рзНржпрж╛ рзй: Slow response / Timeout

**ржХрж╛рж░ржг**: Model ржЦрзБржм ржмржбрж╝ ржЕржержмрж╛ hardware slow

**рж╕ржорж╛ржзрж╛ржи**:
```json
// zombiecoder-config.json ржП
{
  "models": {
    "default": "qwen2.5-coder:0.5b"  // рж╕ржмржЪрзЗржпрж╝рзЗ ржжрзНрж░рзБржд
  },
  "performance": {
    "responseTimeout": 60000,  // 60 seconds
    "streamTimeout": 120000
  }
}
```

ржЕржержмрж╛ ржжрзНрж░рзБржд model ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи:
- `gpt-3.5-turbo` тЖТ `qwen2.5-coder:0.5b` (ржжрзНрж░рзБрждрждржо)
- `gpt-4o-mini` тЖТ `qwen2.5-coder:0.5b`

---

### рж╕ржорж╕рзНржпрж╛ рзк: "Prompt is required" error

**ржПржЗ рж╕ржорж╕рзНржпрж╛ ржЗрждрж┐ржоржзрзНржпрзЗ ржарж┐ржХ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ!**

ржпржжрж┐ ржПржЦржиржУ ржжрзЗржЦрж╛ ржпрж╛ржпрж╝:
- Backend restart ржХрж░рзБржи
- Latest version ржЖржЫрзЗ ржХрж┐ржирж╛ check ржХрж░рзБржи
- `WEBSOCKET_JSONRPC_FIX.md` ржжрзЗржЦрзБржи

---

### рж╕ржорж╕рзНржпрж╛ рзл: Editor connection failed

**Check ржХрж░рзБржи**:
```bash
# Backend health
curl http://localhost:8001/v1/health

# Proxy health
curl http://localhost:5010/proxy/health

# WebSocket (should upgrade to websocket)
curl -i -N -H "Connection: Upgrade" -H "Upgrade: websocket" \
  http://localhost:8001/v1/chat/ws
```

**рж╕ржорж╛ржзрж╛ржи**:
1. Backend ржПржмржВ Proxy ржжрзБржЯрзЛржЗ running ржЖржЫрзЗ ржХрж┐ржирж╛ check ржХрж░рзБржи
2. Firewall block ржХрж░ржЫрзЗ ржХрж┐ржирж╛ check ржХрж░рзБржи
3. Editor configuration ржарж┐ржХ ржЖржЫрзЗ ржХрж┐ржирж╛ verify ржХрж░рзБржи

---

## ЁЯУЪ Documentation Files

| File | Description |
|------|-------------|
| `SETUP_GUIDE_BN.md` | ржПржЗ file - Setup guide |
| `zombiecoder-config.json` | Main configuration |
| `COMPLETE_FIX_SUMMARY.md` | рж╕ржм fixes ржПрж░ details |
| `FINAL_SUMMARY_BN.md` | Bengali summary |
| `PROXY_OPENAI_TEST_RESULTS.md` | Test results |
| `WEBSOCKET_JSONRPC_FIX.md` | WebSocket fix details |

---

## ЁЯОп Performance Tips

### рзз. ржжрзНрж░рзБржд Response ржПрж░ ржЬржирзНржп

- тЬЕ `qwen2.5-coder:0.5b` ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи (default)
- тЬЕ Fake name: `gpt-3.5-turbo` ржмрж╛ `gpt-4o-mini`
- тЬЕ Smaller prompts ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
- тЬЕ Context ржХржо ржжрж┐ржи

### рзи. ржнрж╛рж▓рзЛ Quality ржПрж░ ржЬржирзНржп

- тЬЕ `qwen2.5-coder:1.5b` ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
- тЬЕ Fake name: `gpt-4` ржмрж╛ `claude-3-sonnet`
- тЬЕ Detailed prompts ржжрж┐ржи
- тЬЕ Examples ржкрзНрж░ржжрж╛ржи ржХрж░рзБржи

### рзй. Reasoning ржПрж░ ржЬржирзНржп

- тЬЕ `deepseek-r1:1.5b` ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи
- тЬЕ Fake name: `gpt-4-turbo`
- тЬЕ Step-by-step thinking request ржХрж░рзБржи

---

## ЁЯФТ Security & Privacy

### Data Privacy
- тЬЕ рж╕ржм data local (no cloud)
- тЬЕ ржХрзЛржи external API calls ржирзЗржЗ
- тЬЕ SQLite database (local file)
- тЬЕ рж╕ржорзНржкрзВрж░рзНржг offline ржХрж╛ржЬ ржХрж░рзЗ

### API Keys
- тЬЕ Real API key ржПрж░ ржкрзНрж░ржпрж╝рзЛржЬржи ржирзЗржЗ
- тЬЕ Fake key (`sk-dummy`) ржпржерзЗрж╖рзНржЯ
- тЬЕ Authentication disabled by default

### Network
- тЬЕ рж╢рзБржзрзБржорж╛рждрзНрж░ localhost (127.0.0.1)
- тЬЕ CORS enabled (customize ржХрж░рж╛ ржпрж╛ржпрж╝)
- тЬЕ No telemetry, no tracking

---

## ЁЯУИ System Requirements

### Minimum Requirements
- **CPU**: 2 cores
- **RAM**: 4 GB
- **Disk**: 5 GB free
- **OS**: Windows 10/11, Linux, macOS

### Recommended
- **CPU**: 4+ cores
- **RAM**: 8+ GB
- **Disk**: 10+ GB free
- **GPU**: Optional (faster inference)

### Model Sizes
- `qwen2.5-coder:0.5b`: ~400 MB (ржжрзНрж░рзБрждрждржо)
- `qwen2.5-coder:1.5b`: ~1 GB (balanced)
- `deepseek-coder:1.3b`: ~800 MB (coding)
- `deepseek-r1:1.5b`: ~1.1 GB (reasoning)

---

## ЁЯЖШ Support & Help

### ржпржжрж┐ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝:

1. **Logs Check ржХрж░рзБржи**:
   ```bash
   # Backend logs
   type backend\logs\zombiecoder.log
   
   # Real-time logs
   cd backend && npm run dev
   ```

2. **Health Checks**:
   ```bash
   curl http://localhost:8001/v1/health
   curl http://localhost:5010/proxy/health
   curl http://localhost:11434/api/tags
   ```

3. **Database Reset** (ржпржжрж┐ ржкрзНрж░ржпрж╝рзЛржЬржи):
   ```bash
   # Backup old database
   copy zombi.db zombi.db.backup
   
   # Delete and re-initialize
   del zombi.db zombi.db-shm zombi.db-wal
   node backend/init-db-fixed.cjs
   ```

4. **Complete Reset**:
   ```bash
   # Stop all services
   taskkill /F /IM node.exe
   
   # Clean build
   cd backend
   rmdir /s /q dist
   npm run build
   cd ..
   
   # Restart
   start-zombiecoder.bat
   ```

---

## ЁЯОЙ Success Indicators

ржпржжрж┐ рж╕ржм ржХрж┐ржЫрзБ ржарж┐ржХржарж╛ржХ ржХрж╛ржЬ ржХрж░рзЗ, ржЖржкржирж┐ ржжрзЗржЦржмрзЗржи:

тЬЕ Backend health check: `{"status":"ok"}`  
тЬЕ Proxy health check: `{"status":"ok"}`  
тЬЕ Models list: 15+ models  
тЬЕ Chat response: Within 1-3 seconds  
тЬЕ Streaming: Chunks coming smoothly  
тЬЕ Fake model names: Working perfectly  

---

## ЁЯУЮ Quick Reference

### Essential Commands

```bash
# Start everything
start-zombiecoder.bat

# Check health
curl http://localhost:8001/v1/health

# Test chat
curl -X POST http://localhost:5010/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"Hi"}]}'

# Stop everything
taskkill /F /IM node.exe

# Restart Ollama
taskkill /F /IM ollama.exe
ollama serve
```

### Ports

- Backend: `8001`
- Proxy: `5010`
- Ollama: `11434`
- Admin: `3002` (optional)
- WebSocket: `3003` (optional)

---

## ЁЯЪА Next Steps

1. тЬЕ Setup complete ржХрж░рзБржи
2. тЬЕ Editor/IDE integrate ржХрж░рзБржи
3. тЬЕ Fake model names test ржХрж░рзБржи
4. тЬЕ Performance tune ржХрж░рзБржи
5. тЬЕ Documentation ржкржбрж╝рзБржи
6. тЬЕ Enjoy coding with ZombieCoder! ЁЯзЯтАНтЩВя╕ПЁЯТ╗

---

**Version**: 2.0.0  
**Last Updated**: February 5, 2026  
**Status**: тЬЕ Production Ready  
**Language**: Bengali + English  

**ржЖржорж┐ ZombieCoder, ржпрзЗржЦрж╛ржирзЗ ржХрзЛржб ржУ ржХржерж╛ ржмрж▓рзЗред Happy Coding! ЁЯОЙ**
